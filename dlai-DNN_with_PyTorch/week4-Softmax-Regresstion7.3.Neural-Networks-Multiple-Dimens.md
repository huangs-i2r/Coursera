In this video, we will review Neural Networks that takes in multidimensional input data; we will illustrate this using an example where the input data is two dimensional. In this video we will discuss: Networks with multiple dimensional input, we will focus on the case where the input is 2 dimensional. Constructing Networks with Multiple Dimensional input in PyTorch. We will introduce the problem of Overfitting and Underfitting. In the last video we showed how we can add more neurons to give our neural network more classification ability. In the same way we can add more dimensions to the input, notice that there are a lot more weights between the input layer and hidden layer, we will leave out the bias terms for now. Consider the following samples in two dimensions where the data points are blue for y equals 0, and red for y equals 1. The subsequent line does not separate the two classes. In the same way the following line cannot separate the two classes. Let's look at the problem as a function. In this diagram we use the color to represent the output of the function in the different regions, any point in the red area the function will output 1, and any point in the blue area the function will output 0 this point y hat equals one. Similarly, this point y hat equals one, at this point, y hat equals 0 Similarly, this point y hat equals 0. In this case all these samples here are mis-classified. Using three neurons we will have a different function, everything in the blue region y hat will equal 0, and everything in the red region y hat will equal 1. For example, in the following points y hat has a value of zero. In the red region y hat equals one. We see the function is not linear, but it still appears that the following points are not correctly classified. It appears that four hidden neuronâ€™s appear to do the trick, the following points are classified as 0. The following points are classified as 1. We see that the samples that belong to class zero are in the correct region. We see that the samples that belong to class one are in the correct region. It would be easier to see if we visualize this problem in a higher dimension, here we added an extra dimension to represent y and yhat. So, the surface has a y hat value of zero in the area where the data was labeled with the colour blue. The surface has a y hat value of one in the area where the data was labeled with the colour red. Lets tackle the problem of Multiple Dimensional inputs in PyTorch. We will need to import the following libraries. We will create a data set class as shown in the following image. This is the class for creating our model. We will create a function to train our model, we cumulate the loss iteratively to obtain the cost, we also determine the accuracy using a function see the lab. The process for training is identical to logistic regression on a signal dimension neural network. We create a BCE Loss, we create a dataset object and training loader. We create a our model, we specify two input dimensions and four neurons in the hidden layers. We create an optimizer, then train the model. We can see as the total loss or cost decreases the accuracy gets better. Note that in the lab, you might get one of the 2 versions of the solution shown here. We will introduce the problem of overfitting and underfitting. What are the causes of Overfitting? Overfitting occurs when your model is too complex for the data. Cause of overfitting is when there are too many neurons in the hidden layer. What are the causes of Underfitting? Underfitting occurs when your model cannot capture the complexity of the data. Cause of underfitting is when there are too few neurons in the hidden layer. Lets generate the dataset for our example of overfitting and underfitting The decision function looks like this, the data points on one side of our decision function is red, and the datapoints on the other side of the decision function is blue let's add some noise to our dataset. As you can see, some datapoints are on the wrong side of the decision boundary. When we add some noise to the data points, some samples are not on the correct side of the boundary. In this example, we generate a decision region using a neural network with too many neurons in the hidden layer. You can see that our decision region is too complex and misclassifies some of the datapoints. In this example we generate a decision region using a neural network with too few neurons in the hidden layer. You can see that our decision region is not complex enough to correctly classify some of the datapoints. There are several ways to solve the problem of overfitting and underfitting. Use validation data to determine the optimum number of neurons such that: there is not too many neurons or not too few neurons. Get more data, regularization which will be discussed in later sections.
