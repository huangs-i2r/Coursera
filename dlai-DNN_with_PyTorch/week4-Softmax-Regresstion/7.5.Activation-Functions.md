In this video we will cover the commonly used activation functions that are used in neural networks. We will introduce: Sigmoid, Tanh and Relu activation functions. and how to implement these activation functions in PyTorch. The first activation function is the sigmoid function. Here is the mathematical formula for the sigmoid function. It has an upper-bound of 1 and a lower bound of 0. One of the main drawback with using the sigmoid activation function is the vanishing gradient. Let's take the derivative of the sigmoid function. When z is negative 10 the derivative of the sigmoid function is 0. When z is set to zero, the derivative of the sigmoid function is 0.25 when z is set to 2.5, the derivative of the sigmoid function is 0.07. If you recall from back propagation the derivative of loss with respect to the weight is given by the following mathematical function. The graph here shows the sigmoid function in blue and its derivative in orange. As the product includes the derivative of a bunch of sigmoid functions, this will contribute to the problem of the vanishing gradient, because the product of numbers less than one will tend to go towards zero. The tanh function is another popular activation function. the upper-bound is 1, the lower bound is -1. The tanh function is zero centered as a result it has better performance, see the following paper for details. The following function shows the derivative of the Tanh function in orange and the tanh function in blue. You can see that except when z equals to 0, all the values of the derivative of tanh is less than 1, so the tanh function also suffers from the vanishing gradient problem as well. Our final activation function is the rectified linear unit function or relu function in short. The value of the relu function is 0 when its input is less then zero. If the input z is larger than 0 the input of the function will equal its output. If the input z equals 5, the output equals 5. If the input z equals 10, the output equals 10 otherwise it will equal zero. We will see the gradient as one as well as the input is grater than zero. But when the function is less than zero the derivative is zero. Since the derivative of the relu function is 1 for an input greater than zero, the relu activation function provides a partial solution to the vanishing gradient problem. We can compare all the activation functions in the following plot. Let's implement the sigmoid, tanh, and relu activation function in PyTorch. You can implement the sigmoid function by calling it during the forward pass, taking the linear function as the input. Net_Tanh is our model that implements the tanh activation function. In this case the tanh function is called during the forward pass, taking the linear function as the input. Finally “net relu’ is our model that implements the relu activation function. The relu function is called during the forward pass, taking the linear function as the input. Now we will learn how to construct the neural network model with the tanh and relu activation function using the nn.sequential module. We use the n n dot Tanh object in the.Sequential constructor. This process is similar for the relu, see the lab for examples. Using the same method to load and train the data as we did in multiclass we get the following result. When examining the loss with respect to iteration, we are able to achieve significantly better performance using the relu and tanh activation functions than with the sigmoid activation function. Again when examining the validation accuracy with respect to the number of epochs, we are able to achieve significantly better performance using the relu and tanh activation function than with the sigmoid activation function.
