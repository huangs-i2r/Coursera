In this video we will cover Multi-Class Neural Networks. We will introduce, Multi-Class Neural Networks and (click 2) How to implement Multi-Class Neural Networks in PyTorch How to implement Multi-Class Neural Networks in PyTorch In PyTorch in order to classify multiple classes, you simply set the number of neurons output layer to match the number of classes in the output of the problem. For example, if the output of the problem consists of 3 classes, you should add three neurons to the output layer of your model. Each neuron has its own set of parameters and can be expressed as a row in a matrix. Each neuron has as many input parameters as neurons in the previous layer. We did not include the bias here. Let us use colours to represent each neuron in the output layer of the model. Red for neuron 0, blue for neuron 1, and green for neuron 2. The process to make predictions in multiclass neural networks is similar to using Softmax which we discussed in the previous modules. To illustrate this, let's start an example with three classes, for an input we will we obtain a value in each of the 3 neurons in the output layer. We choose the class with the neuron that has the largest value as the output of the model. In this case neuron two has the largest value, so the output of our model is two. It's helpful to think of the operation in terms of Softmax regression or a linear transformation. Consider an example where we have four neurons in the hidden layer. We replace the features x with the activations of the neurons from the hidden layer. For each class in the output layer; we have a neuron. The column in the matrix represents each neuron. In this case, as we have three classes, our matrix has three columns. As with other linear transforms, we have three biases. The number of rows in the matrix is the number of neurons in the previous layer. In this case, we have four neurons in the previous layer. As a result, we have four rows in our matrix To make a prediction for a given input it's similar to using a Softmax classifier. For each class we get an input, we select the index of the column with the largest value as the class, in this case the class is two. We can also apply the SoftMax method for the two class problem. In this example we have two outputs in the output layer of the neural network. As our models get more complex, we will drop the edges representing the weights and only show the layers and neurons for a simplified overview of the neural network. We have the input vector with D dimensions, followed by the hidden layer, then the output layer. Lets make a Multi-Class Neural Network in PyTorch. The Class to construct a neural network is similar to the one that we used previously. The only modification is that we set the number of neurons in the layer to match the number of classes in our problem, and in addition we drop the activation function in the last layer. Like previously we have the size of the number of input features. We construct a n n dot linear object that we called linear 1 which has the parameter D_in representing the input features. This will be used in the forward function. For the second parameter we have the number of neurons in the hidden layer. This is used as the parameter of our linear 1 and linear 2 object. This will be the number of columns of x in the forward function D out is the number of classes for our problem. This is the other parameter we use to construct the linear 2 object. In the forward function the output x has as many columns as classes. We could also use the nn dod Sequential module to construct our neural network model for the multi class problem. Here input_dim is the the number of input features. Hidden_dim is the number of neurons in the hidden layer, output_dim is the number of classes in the output of the model. In the lab we will be using the MNIST dataset. We will create a validation and training data set object. We will create a validation and training loader. We will use the cross entropy function as the criterion for the loss. The MNIST dataset contains handwritten digits from 0 to 9. We denote the target of the MNIST dataset with the variable y, the variable represents the known classes or labels. Since we have 10 classes representing the digits, y can take on the values from 0 to 9. We convert the image of the handwritten digit to a tensor with 784 dimensions. In the training function we store the training loss and validation accuracy. In this example we calculate the training loss for each iteration. We calculate the accuracy on the validation data for each epoch just like using Softmax by selecting the output with the highest value. When we create model for our neural network, we have the number of input dimensions, the number of hidden neurons. We also input the number of classes, then create the optimizer. We then train the model, we plot the training loss for each iteration. And accuracy for each epoch using the validation data. In the lab we can print out the misclassified samples. For example this sample was misclassified, even for a human its hard to figure out what number it is. In addition to adding an output layer we can add more hidden layers, but it turns out these networks are harder to train. We will learn why next section.
