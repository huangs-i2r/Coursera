In this video we will review how adding more Neurons to the hidden layer will give our model more flexibility. In this video we will review: How adding more neurons in the hidden layer can improve the model Create a Neural Networks with more Neurons in the hidden layer using nn.Module and nn.Sequential. Consider the following samples, if we use the same decision function. The samples between -5 and 5 are misclassified. Shifting the function will not help, in this case most of these samples are misclassified. Scaling will not help as before the samples between -10 and -5 are misclassified. The model is not flexible enough. Ideally we would like to add another function, we can do this by adding more neurons to the hidden layer. Let's see what happens when we multiply the outputs of the activations by the parameters in green. Symbolically we are going to look at the activations. Multiply it by the weights and add them together. Let's use images rather then equations to demonstrate what is going on. Let's look at the following neurons, the output of the first neuron multiplied by the weight will give you the following sigmoid function we then add the output of the second neuron multiplied by the weight Followed by the third neuron multiplied by the weight, the result looks like this. We repeat the process for the next neuron. The output of the 4th neuron multiplied by the weight will give you the following function adding it to the rest of the neurons it begins to look similar to what we need. We add the output of the 5th neuron multiplied by the weight. We add the output of the final neuron multiplied by the weight. The function has the same shape we require but the vertical axis is off. The issue can be solved by applying the sigmoid function, we see the scaling problem is gone, the resulting function is a good candidate to classify the data. Let's see how to build this network with PyTorch. We will need to import the following libraries. This is the class to get our dataset, this is the class for creating our model. We will create a function to train our model, we cumulate the loss iteratively to obtain the cost. The process for training is identical to logistic regression, we create a BCE Loss. We create a dataset and training loader. We create a our model. We specify 6 neurons in the hidden layers, we create an optimizer in the lab we use the adam optimizer which will discuss later then train the model. We can use nn.sequential module as follows, for more accurate prediction we added 7 neurons to the hidden layer. When we graph our model along with the training points, we see that our model makes accurate predictions for the training points.
