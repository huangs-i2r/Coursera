In this video, we're going to discuss back propagation. In particular, we'll do a toy example of back propagation, and we'll discuss the vanishing gradient. We will cover: a toy example of back propagation, and we'll discuss the vanishingmgradient. There's a lot of math in this section, the details are not important all you should take away is: back propagation reduces a number of computations involved in calculating the gradient; and the problem of the sigmoid function i.e., the vanishing gradient. A neural network is a function of another function. The function “z”, is applied to “X” and then the function “a”, is applied to the function “z”. To help illustrate the process we're going to use these set of gears. And we have the gear X, and the gear X rotates at a certain rate, and the gear X affects the gear “A”. But there's also an intermediate gear and we'll denote it by a “Z” So it turns out that the rate of “A” is affected by “Z”, and then by “X”. So the derivative “A” with respect to X, is the derivative of X with respect to Z. Multiplied by the derivative of a with respect to Z. And we call this the chain rule. Let's take a look at this simple toy example of a neural network with one hidden layer and one output layer. We have the cost, let's look at one sample. We will write the output yhat interims of the activation. We have some training data and would like to perform gradient descent to determine the parameters. We need the values of the gradient to perform gradient descent on these two terms. So let's calculate the derivative or gradient. We have a loss function lets just use a squared error in this case. The derivative of the output layer with respect to our first parameter in the output layer. is the derivative of the loss function with respect to the activation on the output layer. Then we have to take the derivative of the activation with respect to the linear input z superscript 2. Then we’ll have to take the derivative of z superscript 2 with respect to our parameter. We now have the derivative of the weight in our output layer. Things get a little more complicated when we want to calculate the derivative of the parameter in the hidden layer. As before we have the derivative of the loss function with respect to the activation on the output layer. Then we have to take the derivative of the activation with respect to the linear input z superscript 2. But instead of taking the derivative with respect to the first weight, we take the derivative of z superscript 2 with respect to the activation of the hidden layer. We then differentiate the activation of the hidden layer with the linear input z superscript 2. Finally we differentiate the z superscript 2 with respect to our final weight and we get the final expression. Back propagation uses the derivative of the first parameter in the output layer, to help us calculate the parameter of the next layer. Here's our two equations we are not interested in the actual equations; we’re just trying to understand the concept. Let's represent similar terms with colors, you can see these two blue terms are present in the output layer and in the hidden layer. We just replace the two blue terms with this red term. We can obtain a computational saving by using the red term from the first equation in the second equation Let's see what happens when we have a deeper network, with more layers, lets calculate the gradient of all the terms. Let's use colors instead of equations. We have the derivative of the output layer and the subsequent hidden layers with respect to the weights. We can represent the blue terms with the light blue term, we can then use this term in the gradient for the parameter in the preceding layer. We then repeat the process for the next two terms, replacing them with a lighter orange. we then use these terms in the gradient for the weight in the deeper hidden layer. Repeating the process for the final layer. In the final layer we have the following terms in our gradient, we can save these terms. Then use them to determine the final gradient. This problem gets much more difficult as we add more layers and neurons, fortunately Pytorch takes care of it all for you using the backwards method. The main problem with building deeper networks is the Vanishing gradient. Consider the following network, let's try to understand the vanishing gradient problem. Let's look at the gradient of the last term. If you look at the expression for the parameter it is a product of many gradients. If you look at the gradient of the activation they are all less than one for any given input As a result if any of the inputs are too large the gradient will be near zero, or the product of these small gradients will be almost zero. Therefore, as we add more layers the gradient will get smaller and smaller. As a result we cannot update the parameter value. To overcome this problem we can change the activation functions or we can learn optimization methods that can help reduce this effect. We will cover these methods in the next video.
