In this video, we'll talk about Softmax prediction. We use the Softmax function when we have a dataset that consists of inputs which can be classified into one of multiple output classes. Up until now, we have classified our data in PyTorch as either belonging to one of the two classes, i.e. class 0 or class 1. In this video, we'll look at how we can use Softmax to classify our input into one of the many classes belonging to the output. Here is how the directed graph of the Softmax function looks like. The graph is identical to that of a multi-output linear regression. The model is identical to a multiple output linear regression. But instead of y, we use z. For classification, we are interested in the column indexes that pertain to the maximum value of z. We'll go more in depth into this in the next video. But for now, let's just get a better idea of how the PyTorch function works. We can apply the Softmax function to multiple input samples as well. Let's talk about implementing the Softmax function in PyTorch. As we saw previously with logistic regression, we can create a custom module for Softmax using the nn.module package in PyTorch. Here's how we would implement the Softmax function in PyTorch if we were implementing it from scratch. If you notice carefully, this implementation is very similar to our custom logistic regression implementation which we had earlier. The only difference is, instead of one, we are now using an out size parameter in the constructor of this class. The parameter corresponds to the number of classes in the output. In addition, there is no logistic function in the output. Let's try to use the Softmax class we just wrote. Here, we use the Softmax constructor for our model. We pass in two as the first parameter since we have a two-dimensional input sample and three as the second parameter since we have three output classes. We'll use our color model to illustrate what's going on. We have our input tensor with two dimensions which we input into our model to get an output z. Z will look something like this. We have the column indexes side-by-side to the actual values of z on the right. We call the max function on z. The argument of one indicates that we want to get the maximum value from z with respect to axis one. In simple terms, this means we want the index pertaining to the max value from z with respect to each column. Since z only has one row, comparing all the columns values of z for only this one row, we see the column with index one corresponds to the largest value in z. In the output, we get the column index corresponding to the largest value in z which, in this case, is one. This would represent the class of the sample x. Let's look at the case when we have multiple multidimensional inputs. Going back to our color model, we can represent this scenario as follows: we pass in x to our Softmax model and get z as the output. Again, here we have the column indexes side-by-side with the actual values of z. We call the max function on z, passing one as the argument which will find the maximum index value with respect to the columns for each row in z. So for the first two samples, the maximum index corresponds to column 1, and for the last sample, the maximum index corresponds to column 2. As a result, each value in yhat would represent the class of each of the corresponding rows in x. Let's do a more detailed example in the next few videos.
