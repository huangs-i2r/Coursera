0:00
In this video, we'll discuss batch normalization. In particular, we'll do an example of batch normalization, we'll discuss batch normalization in PyTorch and we'll go over some of the reasons why batch normalization works. So here's our standard picture of a neural network, and we're only going to look at the case, we'll try batch normalization on the activation before we pass it to the activation function. So let's take a look at the first layer. We have the outputs for each of the neurons and what we're going to do, is we're going to calculate the mean and standard deviation or variance for a particular mini-batch. We're going to normalize the output, then we're going to scale and shift them, and these are actually parameters, which we're going to learn via training. It'll take these outputs and pass them to the activation function. Let's do a little more concrete example. So we have our first mini-batch, which we'll denote by the matrix or Tensor X1, and then we'll have the output for the activations, which will start in the following tensor or matrix. So we'll have the output for the first neuron, second neuron and third neuron. The first sample will correspond to the first row of the matrix of Tensor Z and we'll use the same convention for each of the samples. We'll just use the index m to represent the actual sample. So what we're going to do, is calculate the mean and standard deviation for all the samples for each neuron, and then we'll normalize the output for each neuron. So each [inaudible]. So we calculate the mean for the output for the first neuron, by simply calculating the average of all the samples and repeat the process for the standard deviation. We'll repeat the process for all the outputs of the second neuron, and for the third neuron, and then we'll normalize the output. The output for the first neuron, we'll simply normalize by subtracting the mean and dividing it by the standard deviation. In this is one will add a small little value, in case the standard deviation is zero which is in this case. So normalize the values and we get the following values. We'll repeat the process for the second neuron, and all the ones stay as one and you can verify this by calculating yourself and all the zeros get converted to negative one. We'll repeat the process for all the outputs for the final neuron. Then we'll apply the Scaling ship parameters to each of the neurons and again these parameters are learned via training and this gives a model some flexibility. So in this example, we'll use the following values. We'll take the output of the first neuron and scale and shift all the values using this transfer, same for the second neuron, and the third neuron. Then will repeat the process for the second layer. We'll calculate a mean and standard deviation, we'll normalize the values, apply a second scale and shift parameter, and then we'll pass the activation functions. Repeat the process for the next batch. So we did for X1, so now we'll have the second batch, we'll denote by the matrix or Tensor X2. We'll calculate a new mean and standard deviation and normalize it accordingly, and repeat the whole process again, and we'll do for the second layer as well. So now let's see how to make a prediction. So for the training, we'll calculate the mean and standard deviation for each batch as follows.
Start transcript at 3 minutes 49 seconds3:49
For prediction, we use the population mean and population standard deviation. There's several ways to do this but this is the most simple ways. You can see the [inaudible] documentation, for other ways to calculate it, how the prediction actually works. Procedure is pretty similar. We take our actual activation functions and we apply the same procedure, and as you can see, the mean and variance in this case, don't have the little batch term. We do for the second layer, pretty much the same process and of course these parameters we obtained [inaudible]. So now let's discuss, how to do batch normalization in PyTorch. Custom module or class neural network and we'll create a batch norm object over here. Our input is simply the number of neurons in that particular layer. So in this case, it will be three, and for this layer, it will be four. So we simply apply the batch normalization function to the output in that layer, and I'll perform the normalization, take care of all the training. Similarly for the second layer, it'll do the exact same thing. So to train the model is pretty similar, it might dropout, we'll have to set the model to train. So here are the results and just know you've to use model.evaluates to make a prediction. If you look at train loss, you can see it converges a lot faster and similar results for the test data as well.
Start transcript at 5 minutes 23 seconds5:23
Let's discuss why batch normalization works. We're going to only go over the methods we discussed in this course. So here we have an equation, and we have Z1 and Z2. In this example, the results are constrained from one and in this results, variable z are constrained from 0.1-10. So if you look at the possible values for this equation, you can see that our parameters take on relatively equal values. If you look at this equation, we can see the parameters have all different values they can take. So this expresses itself in the contours of loss function. We use batch normalization. In this case, we can see the contours are relatively round, so we perform gradient descent. The steps are in the relatively same direction and it converges. In this case, we would then use batch normalization, and we'll perform gradient descent. The gradients are pointing in different directions. So it will take us longer to converge them there. So we also have the vanishing gradient problem, and let's say we have this sigmoid function, and we have these inputs, material derivatives. As you can see, these values basically when derive is very similar. If you normalize the data, you can see these values and a range with a gradient doesn't disappear. Through the actual original paper, of batch normalization, it discusses reducing internal covariant shifts. I wouldn't discuss it because its pretty involved but that's another reason why batch normalization works. They also said you don't need dropout in the paper but you can verify that yourself. It allows you to increase the learning rate, and finally the bias isn't really necessary and you can go over the math yourself to verify that. So that's it for batch normalization. That's it.
