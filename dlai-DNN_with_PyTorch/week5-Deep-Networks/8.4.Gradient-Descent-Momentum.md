In this video we will discuss the initialization of the weights in a Neural Network. In this video we will discuss: the problem of not initializing the Weights in a Neural Network model correctly. How to Fix the Problem, Different Initializing methods in PyTorch. Let's see what happens when the weights in the same layer have the same values. Many times when a neural network is not functioning, it has to do with the fact that the weights were not initialized correctly during the training phase of the neural network. Most of the time PyTorch has done the initialization process for us under the hood. Using our simple network, lets see what happens if we do not select good weights to start off with. PyTorch randomly initializes the weights using a method we will discuss later. To understand why this is important let's see what happens when we initialize all of the weights with the same value of one and bias to zero. We will change the model parameters as follows. We will use the simple classification problem. After we train our model, we see that our model does a horrible job of predicting the decision function. Examining the model parameters we see that the weight in the same layer has the same value. For example the linear weights for the first layer for each neuron are the same. Similarly for the second layer. This is because each neuron will have the same output and such have the same gradian update. As a result it is recommended that we randomly initialize our parameters. We initialize our parameters by sampling from the uniform distribution, this distribution has a constant probability in a specified range in this case from -1 to 1. That means any value between -1 to 1 can be sampled with equal probability. A value of -1 may be randomly selected, with equal probability value of 0 may be randomly selected. Also we could select a value of 1, the main problem is how to select the range of the uniform distribution. Our second uniform distribution, has a narrow range between -0.05 to 0.05. The problem is that the values that we sample is squashed between -0.05 and 0.05 which is too close together and defeats the idea of randomly sampling the values of the weights. But there is also a problem with making the range of our distribution, too wide. If we draw samples near the tails of the distribution, the values will be large, this can lead to problems. If you recall if the input to the activation is to large our gradient will vanish, Let's review this by examining the gradient of our squared loss using the tach function. If the gradient with respect to k-th parameter ie k=1 or 2 looks something like this. We can plot out the gradient of the activation function. As you recall for the tanh function, at large values of z or small values of z, the derivative is close to 0. Since the gradient is a product of the derivatives of the activation function, if the derivative of the activation function is close to 0, it will cause the gradient to be close to zero To see what happens if we sample our weights from a distribution, let's look at the worst case. Let's say if we sample two large values and they are the same sign. We sample 1 for w1 and 1 for w2. Let us assume all the activations from the previous layer is 0.5. Let's look at the input to a one neuron, with two inputs, we'll initialize the bias b to zero for this example. The expression for the z is given by, the input of each neuron is 0.5, the weight is 1. the value of z equals one. We can plot the derivative of the activation function. Value of the derivative of the activation function at 1 is about 0.7. Let's see what happens if we had more neurons in the previous layer. Let's see if we use 4 neurons, plugging in a value for the activation of 0.5 and the parameter value of 1 we see the value for z is 2. This is almost twice as large and the value of the derivative is much smaller. If we have 100 inputs, the value for z will be the sum of 100 terms. As a result of the value of z is a large number, as a result the value of the derivative of the activation function is almost zero. We can’t even fit it on the graph, this can cause the problem of vanishing gradients. So we have to adjust the width of the distribution according to the number of inputs. Now lets see How to Fix the problem of the vanishing gradient due to a large number of neurons. The idea for the solution is we scale the width of the distribution by the inverse of the number of neurons. So if we have 2 neurons we scale the width of the distribution by one-half. So the maximum value we can obtain is 0.5. Of course we can obtain samples anywhere in the interval from – ½ to 1/2. If we have 4 neurons as an input we we scale the distribution by one fourth. The maximum value we can obtain is ¼ of course we will not always get this value. Similarly for 6 neurons we scale by 1/6. Let's go over the Different Initialization Methods in PyTorch. Initializing using the Default Method. In PyTorch by default if we use linear to create a linear object with “ L in” neurons, we simply make the lower bound of the range of the distribution the negative of the inverse of square root of L in. the upper bound of the range of the distribution is the positive of the inverse of square root of L in. See the following paper for a more detailed explanation. Initializing using the Xavier Method. Xavier Initialization is another popular method and is used in conjunction with the tanh activation. It takes into consideration the number of input neurons “Lin” As well as the number of neuron in the next layer “L out”. The range of the distribution is given by the following, for more info check out the following paper Once we create the object, we can initialize the weights by applying the function xavier_uniform_ , this will change the state of the objects linear weights. In the lab we compare weights that are initialized from a simple uniform distribution, the PyTorch default and the Xavier’s method. Using the accuracy in the validation data, we see the Xavier method results improve much faster. Initializing using the He Method. For relu we use the He initialize method, after we create a linear object, We use the following method to initialize the weights, for more info check out the following paper. In the lab we compare weights that are initialized from a simple uniform distribution, the PyTorch default and He’s method. Using the accuracy in the validation data, we see the He’s method is much better.
