In this video, we're going to discuss about the dropout method. This method improves the performance of deep neural networks. In particular we will talk about, dropout a method used to prevent over fitting. We will show how to implement the dropout method in PyTorch. Let's say, we have a decision boundary that separates samples into two classes, on one side of the decision boundary we have y=1 which is represented as red, and on the other side of the decision boundary we have y=0 which is represented as blue. In the real world, it is highly unlikely that we have a sample that can be cleanly separated by a decision boundary. The sample here is noisy, that means some data will fall on the wrong side of the decision boundary. Our decision function looks like this, if we use too many parameters in our neural network model, for example, too many layers or too many neurons, we risk overfitting. If our model is overly simplified, we risk underfitting. We could try different combinations of the number of neurons and layers, but it will take too much time and resources. A solution is to start with a model that is complex, and apply a form of regularization call dropout. Dropout is a popular regularization technique use exclusively for neural networks. Dropout involve two phases: The training phase is where we implement the dropout method. And the evaluation phase, where we turn off the drop out method to test the performance of our model. The dropout is implemented by multiplying the activation function with the Bernoulli random variable r. The Bernoulli distribution is a discrete probability distribution where the random variable r takes on the value 0 with probability p, and takes on the value 1 with the probability 1 - p. So if p is 0.5 it's like flipping a fair coin, r=0 with probability p or probability of heads, r=1 with the same probability as tails. Let's see how we implement the dropout method. In the l th layer of the neural network, we multiply. The activation function of each element in the layer with a Bernoulli distributed random variable r. R take on the value 0 with probability p and r takes on the value 1 with probability 1-p. In this case if r subscript 1 equals 0, we are shutting off the first neuron in the layer or the first element in the vector. We have our neural network, we apply dropout to the hidden layers during the forward step while training. We also have a probability p, and this tells us how likely we are to shut off a neuron, each neuron is independent of each other and each iteration. So if one neuron is shut off, it does not affect the probability that the other neurons being shut off in the same layer or another neuron being shut off in another layer. In the Forward step for the first iteration we shut off some neurons in each layer each with a probably of 0.2. This is done by multiplying the activation function by zero. For the second training iteration, we randomly turn several neurons off. As you can see, it is a different set of neurons being turned off in this iteration, than the set of neurons being turned off in the previous iteration. Then we calculate the forward step. The process is repeated for each iteration of training. PyTorch normalizes the values in the training phase of each activation by dividing with the value 1 minus p. This is due to the expected value of each neuron being turned on is 1 minus p. The larger p the more neurons we remove, thus preventing overfitting. If p =0.8 we expect that the majority of neurons to not be active. If p equals one, then all the neurons will be multiplied by zero Generally the more neurons the larger value of p should be used. For the layers with less neurons we use values for p between 0.1 and 0.05. For the layers with more neurons we use a value for p of 0.5. Let's see the evaluation phase. For the evaluation we do not multiply the activation function with the Bernoulli random variable r. We simply run the neural network with all the neurons being active. We see the model with no dropout has regions that overlap the decision boundary. The validation accuracy is 77 percent, we see that the decision function generated by the neural network using a drop out of 0.5 is much less likely to over lap and have an improved validation accuracy of 87 percent. The parameter p is a hyper parameter we can obtain the optimal value of p through cross validation. If the value of p is to small in this case 0.01, we risk overfitting. If the value of p is to large we risk underfitting. The following graph is the cost of the neural network models using the training data and validation data, one model implements dropout and the other does not implement dropout. The line graph in blue does not implement drop out, we see the cost continuously decreases and is the lowest. This suggest that model is catering to the noise of the data. Conversely wee see that as the training cost decreases, the validation cost increases. We see that the model in green uses drop out the training cost is larger than the model without dropout, but the validation cost is smaller. Let's see how to implement Dropout in PyTorch. Here we create our neural network model implementing the dropout method. We set a parameter p in the constructer, that represents the probability. We create a dropout object, the parameter p is the probability of turning off a neuron. We apply the dropout method to the first hidden layer. We then apply the dropout method to the second hidden layer. Here we implement the dropout method for nn dot sequential. This is the class which we use to obtain the data set to generate the polynomial decision boundary. We create a neural network model that implements dropout with a p value of 0.5. The dot train method tells the model that we are in the training phase, which will implement the dropout method, later we use the dot eval method to tell the model it is in the evaluation phase and that will turn off the dropout method. In this lab we will use the ADAM optimizer, this optimizer gives more consistent performance. You can try with SGD. We use our cost or criterion function. We create our dataset, our validation data set, we store some metrics in a Loss dictionary. We train the model, we use batch gradient descent as we can store all the data in memory. We set the model to evaluate to make a prediction on the validation data, we set it back to train when we continue training. Just to reiterate, when the model is in evaluation mode to make a prediction, we would not implement the dropout method. The dot eval method will turn off the dropout method. And thatâ€™s it.
