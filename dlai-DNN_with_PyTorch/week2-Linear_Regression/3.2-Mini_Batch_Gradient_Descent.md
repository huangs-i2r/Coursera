In this video we will cover Mini-Batch Gradient Descent, it has many advantages one important one that it will allow you to process larger datasets, that you will not be able to fit into memory. Because it splits up the dataset into smaller samples. In this video we will review: Basics of Mini-Batch Gradient Descent, Mini-Batch Gradient Descent in PyTorch. In Mini-Batch Gradient Descent we use a few samples at a time for each iteration, its helpful to think about it as if you are minimizing a mini cost function for each iteration. For the first iteration the cost is given by, for the second iteration the cost function is given by In Mini-Batch Gradient Descent, the relationship between batch size, number of iterations and epochs is a little more complicated, let see a few examples, let's start with a batch of 2. Let's use the following boxes to represent the cost or total loss. Lets do the first Epoch, for the first iteration we use the first two samples. For the second iteration we use the second two samples. for the 3rd iteration we use the last two samples. Therefore with a batch size of three, to complete one run or Epoch through the data it took 3 iterations. For the the second Epoch it also takes three iterations. In this case our batch size is two, it only takes two iterations to complete one epoch. For the second epoch it also takes 2 iterations. Let's see how we can determine the number of iterations for different batch sizes and epochs. To obtain the number of Iterations we simply divide the number of training examples by the batch size, let's verify that. For a batch size of one we get 6 iterations, we can verify this pictorially, we see for each iteration we use one sample. For a batch size of 2 it takes three iterations, we can verify this pictorially, each iteration uses two samples. Finally, for a batch size of 3 it takes two iterations. Again we can verify this pictorially. In Pytorch the Process of Mini-Batch Gradient Descent is almost identical to stochastic gradient descent. We create a dataset object, we also create a data loader object. In the parameter we add the dataset object, we simply change the batch size parameter to the required batch size in this case 5. For each iteration, the parameters are updated using five samples at a time. We repeat the process for the next set of samples, the estimated line changes and the loss decreases. We repeat the process for 4 more epochs. We can store the loss value in a list and record it, we can use it to track our model progress, this can be thought of as a approximation to our cost. The following plot show the cost or average loss with different batch sizes. We see that different batch sizes change how long it takes the cost to stop decreasing. This is called the convergence rate.
