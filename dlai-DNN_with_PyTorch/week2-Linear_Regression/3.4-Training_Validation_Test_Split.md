In this video, we will discuss Training data, Validation data and Testing data. Overfitting is when the model fits well with a limited set of data points but does not fit data outside of that limited set such as outliers. Overfitting usually occurs when complex model performs excellently on datasets it was trained on. But performs poorly on datasets that it wasn’t trained on. Consider the following data points generated from the line in blue, The following points do not seem to come from the line, this could be an error or some kind of noise. We split the data into three parts, the training data, this is the data we were using before. We use a part of the dataset as validation data. And then we will use the remaining part of the dataset as test data. To find the best model we randomly split the dataset up, the training data, this is the data we where using before, in this section we will discuss validation data. We also have the test data, the test data tests how your model performs in the real world, We will not discuss this in this video. You use your training data to get Model Parameters via training, for example you get your bias and slope parameters via gradient descent. But there are elements related to your model that you can change , these are called Hyperparameters. (click 2) for example the learning rate and batch size We use our training data to train the model, we have our cost or average loss function. We can view this as a surface, we minimize this via gradient descent. Its sometimes helpful to write it as follows, the super script stars indicate the parameters that maximize the values In order to train the model we require several hyper parameters including Mini-Batch size and learning rate. We use the validation data to determine these parameters, let’s see an example We try two learning rates. We use the first learning rate, we train the model with gradient descent. We get our first model, we try a second learning rate, we train the model. We obtain the second model, if we use the training data we would select the second model. We then calculate the validation data for both models to calculate the cost. Where Nv is the size of the training set, we calculate the cost for model . We calculate the cost for model 2, we select the model that minimize the cost on the validation error. Consider the following example with one sample of validation data. The value of y1 is 15 and the value of x is zero. For one sample the equation for the cost on the validation data simplifies to. We calculate the cost for the first model, we get a value of 256. We calculate the cost for the second model, in this case the loss is zero. As result we use the second model. In the following plot we see the cost for the training data in blue and the validation data in orange. The x axis represents the different learning rates Using the test data does not always generate the best values, the following plot shows the validation data points in red and different line generated with different learning rates. We see the line that corresponds to the minimum cost for the test data is actually the one with the peak cost for the validation data. The line estimated using this learning rate is not a good fit for the generated points. We see the line that corresponds to the minimum of the validation data actually the peak of the training loss. But the estimated line is much closer to the test data. Just a reminder that usually the split is done randomly and in this case we performed the split deterministically to make the results easier to understand.
