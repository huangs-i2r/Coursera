#### Linear Regression Training

In this video we will go over the process of learning parameters for linear regression this is called training. In this module we will review, what is a dataset, the noise assumption, provide an overview of training. We use a data set of examples, in this case we have N points with x and y values, we will use these examples to learn the linear relationship or line between x and y. When x is only one dimension linear regression is sometimes referred to as simple linear regression. Imagine this set with many thousands of ordered pairs. Corresponding x, y coordinates are marked with the same subscript, linking them together. Subscripts, range from n = 1 to N with N defining the set size. Commonly you will encounter datasets organized as tensors. Example of simple linear regression datasets include: Predicting housing prices giving the size of the house. The variable y is the house price and x is the size Predicting stock prices using interest rates. In this case y is the stock price and x is the interest rate. Fuel economy of cars give horsepower, y is the fuel economy and x is the horsepower Even if the linear assumption is correct, there is always some error, we take this into account by assuming a small random value is added to the point on the line, this is called noise. For linear regression, the particular type of noise is Gaussian. The figure on the left shows the distribution of the noise. The horizontal axis shows the value added and the vertical axis illustrates the probability that the value will be added. usually, a small positive value is added or a small negative value Sometimes large values are added, but for the most part, the values added are near zero. The more significant the standard deviation or, the more disperse the distribution is, the more the samples deviate from the line. In linear regression we plot the points on the cartesian plane. We would like to come up with a linear function for x that can best represent the points. In this example the line does not do a good job. This line does a slightly better job at fitting the points. Finally, this line does the best at fitting all the points. A more systematic way of finding the best line can be determined by minimizing a function The following is the average loss Mean squared error or cost function, it is a function of the slope and bias. As we plug in different slope and biases we get different values, it turns out the line with the best fit has the smallest value for this function. Now lets see how to minimize this cost.

#### Loss

In the video we will review Loss. Loss is a precursor to cost. In order to determine the parameters, we have our linear function, we would like to determine the values of the parameters i.e the slope and bias. Let's start off with a simple example with just one sample as shown in the graph. The value for x is -2 and the value y is 4 We would like to come up with the slope and bias. Actually this is an overdetermined problem, i.e there are an infinite number of solutions, in order to make the problem easier let's just try to determine the slope. In order to find the parameter we need to find how good our model is, a quantity that is near zero when our model provides a good estimate and large when our model estimate is bad, we call this quantity the loss. We do this by subtracting our model estimate by the actual value, we then square it. Its essentially finding the distance from the model estimate from the actual value we are trying to predict. We would like to determine the parameter, in this case, the slope that minimizes this value, this is a function of our parameter. In the training phase, y and x are given. In the final form, our function looks like this, It's referred to as a criterion function or loss function, the goal is to determine the parameter or slope that minimizes this function Generally loss is a function that takes your true input. And your predicted or estimated input. It then provides you with a number that lets you know how good your estimate is. It's helpful to think of the loss as a function of the parameter you would like to learn. The table on the right shows some possible values of our parameter and the value of the loss generated. We see the line created for these values. We see the closer the line gets to the point the smaller the loss becomes. It's difficult to randomly get values so let's come up with a more systematic way of minimizing the error. Itâ€™s helpful to display the loss function on the right, it's shaped like a concave bowl. This is referred to as the parameter space, the left contains different lines corresponding to different parameters Selecting a slope of 5 we see the line is far from the data point. In the data space the value of the loss function is relatively large, selecting a slope of 1 we see the value for the loss is near the minimum of the parameter space. Selecting a slope of -1 the result gets much closer to the minimum of the loss function and we are much closer to the loss curve. A slope of -5 we see we are at a much higher point on the loss curve and the line is much farther away from the data point. Therefore we would like to find the minimum value of the loss function. If wee look at the derivative of the loss function we see there is a negative value on the left side of the minimum and a positive value on the right side of the minimum. Finally, there is a zero at the minimum We can actually find the best value for the slope by setting the derivative = 0. We find the algebraic expression for the derivative. Do some algebra, and we get the best value for the slope, but we will not be able to do this for more complex deep learning models. But we can still use the derivative to help us find the minimum.
