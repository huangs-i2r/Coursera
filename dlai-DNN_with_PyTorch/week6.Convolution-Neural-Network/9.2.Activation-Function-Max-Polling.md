In this video we're going to discuss activation functions and max pooling. First we will talk about the activation functions, then we'll discuss max pooling. Let's talk about activation functions. Using the Activation Map we apply the activation function to each element in the activation map. We get an output tensor that is the same size and shape. We have the input image tensor X and perform convolution and get the output tensor Z, weâ€™ll then apply the activation function. Let's do an example. We will have our input image we will have our kernel. We take our input image, we perform convolution, we have our activation map we apply the activation function. Let's look at the output. We have negative 4, we'll apply the relu function, we get zero. This applies to the other elements in the column, we'll apply the relu function, we get all zeros. For the second column we have all zero values, we'll apply the relu function, we get all zeros. For the final column we have all 4's, we'll apply the relu function, we get all 4s as the output. When the activation function is applied to multiple channels, we simply apply the activation function to each element. We can see in this example all the negative values are set to zero and the other values remain the same. In PyTorch to apply the activation function, we have an image, we perform convolution, we then apply the activation function. If you would like to use the sequential method we would create a relu object and apply it. Now lets apply Max pooling, we will review one of several variants. Max pooling will reduce the size of the activation maps reducing the number of parameters, it will also reduce the impact of small changes in the image. Remember our last diagram, we usually apply Max pooling to the activation map. We have an image, we have a region of pixels of shape K by K. We simply choose the maximum value in the proscribed regions, we have the following region. we simply select the pixels with the largest value. We shift the region and repeat, just like convolution we repeat the process.
Start transcript at 2 minutes 33 seconds2:33
We stop when we get to the end of the image. In addition, we can create a max pooling object and apply it to the image, with the region size and stride, the result is a new tensor, the shape can be determined just like convolution. We can also apply max pooling directly to the image as a function. If we set the stride equal to none, the default parameter we shift the entire area per stride, we get the following outputs.
Start transcript at 3 minutes 6 seconds3:06
We continue moving.
Start transcript at 3 minutes 10 seconds3:10
We can represent the pooling as follows. Max pooling helps by making small changes in the images have less impact. We have two images they are identical but one slightly shifted, we apply max pooling. We start shifting.
Start transcript at 3 minutes 32 seconds3:32
We see the output is identical now. And that's it!
