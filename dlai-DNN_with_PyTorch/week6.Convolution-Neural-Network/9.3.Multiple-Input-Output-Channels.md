In this video, we're going to discuss convolution with multiple channels. We'll talk about multiple output channels, multiple input channels, and multiple input and output channels. First, we'll discuss multiple output channels. As you recall in the first case, we had the image or tensor X, we performed convolution with a kernel, and then we get an activation map. For multiple output channels, we have multiple kernels. Each kernel will produce a different activation map. We create a convolution object, we specify the number of output channels in this case three. We will create an image, these two parameters here will be the number of images, or mini batch size, and the second element will be the number of channels, in this case 1. We perform the convolution, just to note, because PyTorch randomly initializes our tensors, we're going to set some nice values up using convolution kernels people have already come up with before. We have an input image X, we then perform three convolutions, each have three outputs, denoted by Z0, Z1, and Z2. Let's convert the tensor into an image of a vertical line and look at the outputs. We perform the convolution and for each channel we have an output. If you look at these two activation maps they are basically a constant value. If you look at the result of this activation map it has a different output, this kernel can actually detect the edge. In general we can see that certain kernels can be used to detect certain features in an image. Let's see what happens when we apply a different image. We will create a second image, we will perform the convolution using the same kernels. We have the image it’s a horizontal line. Let's take a look at the output channels. The output of this channel detected the horizontal line. Essentially this kernel is actually a horizontal line detector. For the other two kernels nothing happened. To reiterate, different kernels detect different features in the image. In general, if we have three output channels when we perform convolution, every channel will have its own independent kernel and bias term as well and output three activation maps. The kernel channel is the first element in the shape attribute for each output. For the output the first axis corresponds to the sample and the second axis corresponds to each activation map. Now, let's discuss multiple input channels. You might have multiple input channels in an RGB image, where each channel represents a different intensity of red, green, and blue. Usually We'll have multiple channels in a convolutional neural network. This should not be confused with 3D convolution as each convolution is still 2D. For multiple input channel convolution, we’ll have one output, and for every input channel we'll have our own kernel k, we’ll perform convolution with that kernel with the corresponding input channel, we'll add the results together, and we'll get an output. A matrix analogy is helpful, where our parameters are like a row vector and our input images are like a column vector, we perform the dot product operation, but instead of performing multiplication we’ll perform convolution. We preform convolution with the first kernel and image, then the second kernel and channel we add the results and get the output. Let's do an example with two input channels, we will use the following values for the kernel the bias will be zero. Here’s the image it will have two channels, for channel 1 here is the image or tensor, also for channel 2. We will perform convolution on the first channel, then we'll perform convolution with the second channel. we will then add the results for each convolution together. Now we'll discuss multiple-input and output channels. Let's create a convolution object with 2 input channels, we represent each channel with a block with 3 output channels. Each output channel will have two sets of kernels, for each input channel. For the first output we have these two kernels, and we take our two inputs and we perform convolution with those two kernels, we add the results together we then have the final output. For the second output channel we do the exact same thing, we take the two inputs with each of the corresponding kernels, we perform convolution, add up the results, we get the second output channel. For the final output, we have two kernels each for the two inputs, we perform the convolution and add the results together, we then get the final output. Its helpful to think of the process like matrix multiplication. The kernels are elements in the matrix, the number of inputs are the number of columns and the number of outputs are the number of rows. We have two inputs represented by a two element column vector. We perform the multiplication and we get three outputs. Here's the formula for the multi-channel convolution, where L is the output channel and K is the input index. Let's look at an actual example, let's see how the actual different kernels look like. Just a note, PyTorch randomly initializes the kernels, but in the lab we specify custom values for you to use. We have the kernels for the first input channel, the kernels for the second input channel, and the kernels for the third input channel. We'll create an input image, the first channel will be the following image, the second channel will be the following image. Let's see the output for the different channels. Here's our equation again and we'll just write down the expression for the first channel, we’ll set the bias term equal to zero. For the 1st output, for the first channel we convolve it with the first kernel, we do the same with the second kernel and second input. We add the results of each operation, we get an activation map. For the second output channel we repeat the process convolving the two channels. ( We add the activation map, getting the second output. We do the same for the final channel. And that's it!
