In this video, we're going to discuss convolutional neural networks. CNN for short. We will discuss, The Convolutional Neural Network Constructor, The Forward Step, Training in PyTorch. This is the standard picture you might see for a convolutional neural network. It looks similar to a regular network, in this example we have one hidden layer, we have one output layer we have the convolution kernels. Like the hidden and output layer parameter these theyâ€™re obtained via training we have the Activation Map where we can apply the activation functions. Then we have the pooling layer applied to each channel, this diagram can get a little complicated so let's build an extremely simple cnn. Let's start off with a really simple example. We are going to try to distinguish between a horizontal line and a vertical line. In the vertical line, Y will equal 0, and in the horizontal line, y will equal 1. To make things interesting, we're going to add some noise. The next sample for y = 0 might look like this, also the next sample for y=1 may look like this. Let's build a simple CNN, with two convolution layers and an output layer. We have the image X, lets remove it to make everything more clear. Our first convolutional layer we will have two channels. We are going to apply the convolution operation we then get our activation map for each kernel, we then apply the activation function. We will then apply pooling, we now have two outputs. For our second layer we'll have two inputs and one output, this will correspond to two kernels. We'll convolve the outputs of the preceding layer with its own kernel, and add the results together, the result is one activation map. After the final step, we apply an activation function and max-pooling to produce an output. this is the output of our second convolutional layer. We simply flatten or reshape this output of the max pooling layer, for example if the output of the max pooling layer is 7 units of width and 7 units of height, we flatten or reshape the tensor to a 1D tensor with 49 components. Sometimes calculating the shape of this output is the hardest part. This is then used as in input to a fully connected neural network. In this lab we will use one output layer. In summery, we have two convolution kernels these are parameters that we get via training. The activation map where we apply the activation function and max pooling, a second set of kernels as before these are parameters that we get via training, the second activation map where we apply the activation function and max pooling, we flatten this output and use this as an input to a neural network. This can get confusing so lets look at an other view to get a better understanding. We have the two channels corresponding to our first convolution layer, the second two channels corresponding to our second convolution layer, We will apply the activation function and max pooling to the first activation map. We take the two outputs from our first layer, convolve them together and add the results. We will apply the activation function and pooling to the activation map then we flatten this output, this is used as an input to the fully connected layers to make a classification. As it gets difficult to calculate the shape of the network we will only include two parameters, the number of output channels for our first convolution layer, the number of output channels for our second convolution layer. Let's review the Convolutional Neural Network Constructor. Let's review the object constructor for the cnn . We produce a 2d convolution object cnn1. As we are going to use gray scale images we will only have one input channel. The number of output channels in this case two, we also include the kernel size and padding size. We add a max pooling object with kernel size and stride size, we will include the activation function in the forward step. We then add the second convolution layer, the number of inputs for the second layer is equal to the number of outputs for the first layer, and the number of outputs for the second layer will be 1, we will also include the stride, size and padding size. We will add a second max pooling layer. We will add an output linear layer, we can determine the shape of the output channel using the formulas we used in the previous section. For this, it will be 7 by 7 as a result when we flatten the output it will have 49 elements. As a result our final layer will include two neurons one for each class, each neuron will have an input dimension of 49. Lets review theForward method This is the Forward Method. In the forward step, we apply the convolution. We then apply the activation function and max pooling for the first layer and assign it to x. We take the output x, we the apply the second convolution layer we then apply the activation function and max pooling, we now have an output. We reshape the output using the method view, this converts our rectangle input to a 1d input. We then apply the linear layer. Training in PyTorch, we can use back propagation to update the parameters including the two convolution layers and the hidden layers. Training is identical we create our data set object and our cnn model. The criterion function, our optimizer, some parameter's for training, our train loader and validation loader. We then train the model, see the lab for more details. We can see as the cost decreases the accuracy goes up, check the lab for more details.
